{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Chapter 1: Quadratic optimization\n",
    "\n",
    "*Selected Topics in Mathematical Opimization*\n",
    "\n",
    "**Michiel Stock** ([email](michiel.stock@ugent.be))\n",
    "\n",
    "![](Figures/logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Quadratic systems are important:\n",
    "\n",
    "- Systems close to their maximum can be closely approximated by a quadratic system, studying the minimization of quadratic systems can teach us about minimization of general convex functions.\n",
    "- Quadratic systems are important in their own right! Many statistical models, graph problems, molecular models etc. can be formulated as quadratic systems:\n",
    "  - least-square minimization problems\n",
    "  - inference using multivariate normal distributions\n",
    "  - molecular modeling using spring-mass systems\n",
    "  - signal recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warming up: one-dimensional quadratic systems\n",
    "\n",
    "In the the scalar case, a quadratic function is given by\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{2}px^2+qx +r\\,,\n",
    "$$\n",
    "\n",
    "with $p>0$ (we will shortly see why).\n",
    "\n",
    "Our optimization problem is given by:\n",
    "\n",
    "$$\n",
    "\\min_x\\,\\frac{1}{2}px^2+qx +r\\,.\n",
    "$$\n",
    "\n",
    "This can easily be solved by setting the first order derivative equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d}f(x)}{\\mathrm{d}x} = px + q \\\\\n",
    "px^\\star+q = 0 \\Leftrightarrow x^\\star=\\frac{-q}{p}\\,.\n",
    "$$\n",
    "\n",
    "To show that this is the sole minimizer of $f(x)$, we have to prove that the second order derivative is positive in this point. This means that at that point the derivative of the function is increasing: a little to the left the function is increasing and a little to the right and the function is decreasing. We have\n",
    "$$\n",
    "\\left.\\frac{\\mathrm{d}^2f(x)}{\\mathrm{d}x^2}\\right|_{x^\\star} = p\\,,\n",
    "$$\n",
    "so if $p>0$ then $x^\\star$ is the minimizer of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 1**\n",
    "Complete the code for solving the 1-D quadratic system. Use it to find the minimum of\n",
    "$$\n",
    "4x^2+8x + 2\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# solutions are given in quadratic.py\n",
    "from quadratic import solve_1d_quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_1d_quadratic(p, q, r=0):\n",
    "    \"\"\"\n",
    "    Finds the minimizer of an 1-D quadratic system,\n",
    "    raises an error if there is no minimizer (p<0)\n",
    "\n",
    "    Inputs:\n",
    "        - p, q, r: the coefficients of the 1D quadratic system\n",
    "\n",
    "    Output:\n",
    "        - xstar: the minimizer\n",
    "    \"\"\"\n",
    "    assert ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve 1-D quadratic system here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards $n$-dimensional quadratic systems\n",
    "\n",
    "Let us directly move from the one-dimensional case to the $n$-dimensional case. We will use vector notation:\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix}\n",
    "       x_1 \\\\ \\vdots \\\\ x_n\n",
    "     \\end{bmatrix} \\in \\mathbb{R}^n\\,.\n",
    "$$\n",
    "A general $n$-dimensional linear system is given by:\n",
    "$$\n",
    "f(\\mathbf{x}) =\\frac{1}{2} \\mathbf{x}^\\top P \\mathbf{x} + \\mathbf{q}^\\top\\mathbf{x} + r\\,,\n",
    "$$\n",
    "with $P$ an $n\\times n$ symmetric matrix, $\\mathbf{q}$ an $n$-dimensional vector and $r$ a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "Why is $P$ symmetric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to solve the problem:\n",
    "$$\n",
    "\\min_\\mathbf{x}\\,\\frac{1}{2}\\mathbf{x}^\\top P \\mathbf{x} + \\mathbf{q}^\\top\\mathbf{x} + r\\,.\n",
    "$$\n",
    "\n",
    "The concept of a derivative is extended towards higher dimensions using the *gradient* operator:\n",
    "$$\n",
    "\\nabla_\\mathbf{x} = \\begin{bmatrix}\n",
    "       \\frac{\\partial \\, }{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial \\, }{\\partial x_n}\n",
    "     \\end{bmatrix}\\,,\n",
    "$$\n",
    "so that the gradient of $f(\\mathbf{x})$ is given by:\n",
    "$$\n",
    "\\nabla_\\mathbf{x} f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "       \\frac{\\partial f(\\mathbf{x}) }{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f(\\mathbf{x}) }{\\partial x_n}\n",
    "     \\end{bmatrix}\\,.\n",
    "$$\n",
    "From now on, we will drop the subscript in the gradient when clear from context. For those not familiar to vector calculus, the most useful rules are given below. Here, $a, b$ and $c$ are scalars, $f$ and $g$ are arbitrary differentiable functions and $\\mathbf{b}$ is a vector.\n",
    "\n",
    "| rule | example     |\n",
    "| :------------- | :------------------------------------------------ |\n",
    "| linearity      | $\\nabla_\\mathbf{x}(a f(\\mathbf{x}) +b g(\\mathbf{x})) = a\\nabla_\\mathbf{x} f(\\mathbf{x}) +b\\nabla_\\mathbf{x} g(\\mathbf{x})$       |\n",
    "| product rule | $\\nabla_\\mathbf{x}(f(\\mathbf{x}) g(\\mathbf{x})) = g(\\mathbf{x})\\nabla_\\mathbf{x} f(\\mathbf{x}) + f(\\mathbf{x})\\nabla_\\mathbf{x} g(\\mathbf{x})$|\n",
    "|chain rule| $\\nabla_{\\mathbf{x}} f(g(\\mathbf{x})) = \\frac{\\partial f}{\\partial g}\\mid_{g(\\mathbf{x})} \\nabla_\\mathbf{x} f(\\mathbf{x})$|\n",
    "| quadratic term | $\\nabla_\\mathbf{x} \\left(\\frac{1}{2}\\mathbf{x}^\\top A\\mathbf{x}\\right)= A\\mathbf{x}$|\n",
    "|linear term| $\\nabla_\\mathbf{x} (\\mathbf{b}^\\top\\mathbf{x})=\\mathbf{b}$|\n",
    "|constant term |$\\nabla_\\mathbf{x} c = 0$ |\n",
    "\n",
    "The gradient of the quadratic function is\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})=P\\mathbf{x} +\\mathbf{q}\\,.\n",
    "$$\n",
    "Setting this to zero gives\n",
    "$$\n",
    "\\mathbf{x}^\\star=-P^{-1}\\mathbf{q}\\,.\n",
    "$$\n",
    "\n",
    "> *Even though the solution contains the inverse of a matrix, it is seldom a good idea to compute a matrix inverse. Instead, use a solver for the linear system $A\\mathbf{x}=\\mathbf{b}$ (numerically stable).*\n",
    "\n",
    "How do we know that $\\mathbf{x}^\\star$ is the minimizer of the quadratic system? For this we have to extend the concept of a second order derivative to $n$ dimensions. We define the *Hessian* as:\n",
    "$$\n",
    "\\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f(\\mathbf{x})}{\\partial {x_{1}^2}} & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 x_2} & \\ldots &  \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 x_n}\\\\\n",
    "\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 x_2} & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial {x_2}^2} & \\ldots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 x_n} & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_2 x_n} & \\ldots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_n^2}\n",
    "\\end{bmatrix}\\,.\n",
    "$$\n",
    "For the quadratic system, this boils down to\n",
    "$$\n",
    "\\nabla^2 f(\\mathbf{x}) = P\\,.\n",
    "$$\n",
    "The condition for $\\mathbf{x}^\\star$ to be the minimizer of $f(\\mathbf{x})$ is that the Hessian should be *positive-definite* in that point.\n",
    "\n",
    "> A symmetric $n\\times n$ matrix $A$ is positive-definite (in symbols: $A\\succ0$), if for any vector $\\mathbf{z}\\in\\mathbb{R}^n$\n",
    "> $$\n",
    "> \\mathbf{z}^\\top A \\mathbf{z} > 0\\,.\n",
    "> $$\n",
    "\n",
    "A matrix is positive-definite if (and only if) all its eigenvalues as positive.\n",
    "\n",
    "A point $\\mathbf{x}^\\star$ at which the gradient vanishes is a minimizer if and only if\n",
    "$$\n",
    "\\nabla^2 f(\\mathbf{x})|_{\\mathbf{x}^\\star} \\succ 0\\,.\n",
    "$$\n",
    "\n",
    "So, for the quadratic problem, $x^\\star$ is the unique minimizer iff $P\\succ 0$. This means that along every direction $\\mathbf{v}\\in \\mathbb{R}^n$ to project $\\mathbf{x}$, the problem reduces to a one-dimensional quadratic function with a positive second-order constant:\n",
    "$$\n",
    "x_v = \\mathbf{v}^\\top \\mathbf{x}\\\\\n",
    "f'(x_v) = x_v (\\mathbf{v}^\\top P \\mathbf{v}) x_v + (\\mathbf{v}^\\top \\mathbf{q})x_v + r\\,,\n",
    "$$\n",
    "where $\\mathbf{v}^\\top P \\mathbf{v}>0$ if $P\\succ 0$, which in turn implies that $f'(x_v)$ has a minimizer.\n",
    "\n",
    "If $P\\succ 0$, the quadratic system is a *convex* function with a single minimizer. In many problems, $P$ is positive-definite, so there is a well-defined solution. We will develop this further in Chapter 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 2**\n",
    "\n",
    "Complete the code for solving the $n$-D quadratic system. Use it to find the minimum of\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{x}^\\top\\begin{bmatrix}4 & 1 \\\\ 1 & 2\\end{bmatrix}\\mathbf{x} + \\begin{bmatrix}3 \\\\1\\end{bmatrix}^\\top\\mathbf{x} + 12\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from quadratic import solve_nd_quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nd_quadratic(P, q, r=0):\n",
    "    \"\"\"\n",
    "    Finds the minimizer of an N-D quadratic system,\n",
    "    raises an error if there is no minimizer\n",
    "    (P is not positive-definite)\n",
    "\n",
    "    Inputs:\n",
    "        - P, q, r: the terms of the nD quadratic system\n",
    "\n",
    "    Output:\n",
    "        - xstar: the minimizer, an (n x 1) vector\n",
    "    \"\"\"\n",
    "    assert np.all(np.linalg.eigvalsh(P) > 0)\n",
    "    return ...  # to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the system\n",
    "xstar_exact = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Consider $L_2$ regularized ridge regression:\n",
    "$$\n",
    "\\min_\\mathbf{x}\\, (\\mathbf{y} - B\\mathbf{x})^\\top(\\mathbf{y} - B\\mathbf{x}) + c\\cdot \\mathbf{x}^\\top\\mathbf{x}\\,,\n",
    "$$\n",
    "with $c>0$. Write this in the standard form of a quadratic system and show that it is convex. Give the expression for the minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time and memory complexity of exact solution\n",
    "\n",
    "The exact solution for convex quadratic system hinges on solving a $n\\times n$ linear system. Conventional solvers for linear systems have a time complexity of $\\mathcal{O}(n^3)$. This is doable for problems of moderate size ($n<1000$), but becomes infeasible for large-scale problems (on a standard computer).\n",
    "\n",
    "Storing an $n\\times n$ matrix also has a memory requirement of $\\mathcal{O}(n^2)$. When $n$ is too large, this cannot fit in main memory. In the remainder of this chapter, we will consider the case when $P$ is too large to work with, while matrix-vector products $P\\mathbf{x}$ *can* be computed. Some examples of when such settings occur:\n",
    "\n",
    "- $P=B^\\top B$, with $B\\in \\mathbb{R}^{n\\times p}$, with $p\\ll n$.\n",
    "- $P$ is a very sparse matrix.\n",
    "- $P$ has a special structure so that $P\\mathbf{x}$ can be computed on the fly, e.g. $P_{ij}=i^2j^3$.\n",
    "- $P$ is a sparse block matrix (blocks can be loaded and processed independently)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descent methods\n",
    "\n",
    "Instead of computing the solution of a convex quadratic system in one step, we will use *descent methods*. Here, a minimizing sequence $\\mathbf{x}^{(k)},\\, k=1,\\dots$, where\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} +t^{(k)}\\Delta \\mathbf{x}^{(k)}\\,,\n",
    "$$\n",
    "with $t^{(k)}\\geq 0$ called the *step size* (in machine learning often called *learning rate*) and $\\Delta \\mathbf{x}^{(k)}$ called the *search direction*. Proper descent methods have that\n",
    "$$\n",
    "f(\\mathbf{x}^{(k+1)}) < f(\\mathbf{x}^{(k)})\\,,\n",
    "$$\n",
    "except when $\\mathbf{x}^{(k)}$ is optimal. For this property to hold, the search direction should satisfy\n",
    "$$\n",
    "(\\Delta \\mathbf{x}^{(k)})^\\top \\nabla f(\\mathbf{x}) < 0\\,.\n",
    "$$\n",
    "\n",
    "![Descent and ascent step.](Figures/descent_step.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General descent algorithm\n",
    "\n",
    "Below is the general pseudocode of a general descent method:\n",
    "\n",
    "> **given** a starting point $\\mathbf{x}$\n",
    ">\n",
    "> **repeat**\n",
    ">> 1. Determine descent direction $\\Delta \\mathbf{x}$\n",
    ">> 2. *Line search*. Choose $t>0$.\n",
    ">> 3. *Update*. $\\mathbf{x}:=\\mathbf{x} + t \\Delta \\mathbf{x}$.\n",
    ">\n",
    "> **until** stopping criterion is reached.\n",
    "\n",
    "Usually, the convergence criterion is of the form\n",
    "$$\n",
    "||\\nabla f(\\mathbf{x})|| < \\nu\\,.\n",
    "$$\n",
    "\n",
    "The step size can be chosen in several ways:\n",
    "\n",
    "- **exact**: $t=\\arg\\min_{s\\geq 0}\\, f(\\mathbf{x}+s\\Delta \\mathbf{x})$.\n",
    "- **approximate**: choose a $t$ that only approximately minimizes $f(\\mathbf{x}+s\\Delta \\mathbf{x})$.\n",
    "- **decaying**: choose some decaying series, e.g. $t = \\frac{1}{\\alpha+k}$.\n",
    "- **constant**: a constant step size (often works fine in practice).\n",
    "\n",
    "For quadratic systems we can compute the exact step size, as this amounts to a simple one-dimensional quadratic problem:\n",
    "$$\n",
    "t=\\arg\\min_{s\\geq 0}\\, \\frac{1}{2}(\\mathbf{x}+s\\Delta \\mathbf{x})^\\top P (\\mathbf{x}+s\\Delta \\mathbf{x}) + (\\mathbf{x}+s\\Delta \\mathbf{x})^\\top \\mathbf{q} + r\n",
    "$$\n",
    "$$\n",
    " =\\arg\\min_{s\\geq 0}\\, \\frac{1}{2}s^2(\\Delta\\mathbf{x})^\\top P \\Delta\\mathbf{x} + s((\\Delta \\mathbf{x})^\\top P\\mathbf{x}+(\\Delta \\mathbf{x})^\\top\\mathbf{q}) +\\text{constant}\n",
    "$$\n",
    "\n",
    "This can be solved as:\n",
    "\n",
    "$$\n",
    "t = \\frac{-(\\Delta\\mathbf{x})^\\top P \\mathbf{x}-(\\Delta\\mathbf{x})^\\top\\mathbf{q}}{(\\Delta\\mathbf{x})^\\top P \\Delta\\mathbf{x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 3**\n",
    "\n",
    "Complete the code for the exact line search for quadratic systems."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from quadratic import quadratic_exact_line_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_exact_line_search(P, q, Dx, x):\n",
    "    \"\"\"\n",
    "    Find the exact step size that minimized a quadratic system in\n",
    "    a given point x for a given search direction Dx\n",
    "\n",
    "    Inputs:\n",
    "        - P, q: the terms of the nD quadratic system\n",
    "        - x: starting point\n",
    "        - Dx: search direction\n",
    "\n",
    "    Output:\n",
    "        - t: optimal step size\n",
    "    \"\"\"\n",
    "    t = ...  # to complete\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "A natural choice for the search direction is the negative gradient:\n",
    "$$\n",
    "\\Delta \\mathbf{x} = - \\nabla f(\\mathbf{x})\\,.\n",
    "$$\n",
    "Remember, for the quadratic system, the gradient was\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})=P\\mathbf{x} + \\mathbf{q}\\,,\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\Delta \\mathbf{x} = - P\\mathbf{x} - \\mathbf{q}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm for quadratic systems\n",
    "\n",
    "> **given** a starting point $\\mathbf{x}$\n",
    ">\n",
    "> **repeat**\n",
    ">> 1. $\\Delta \\mathbf{x} := - P\\mathbf{x} - \\mathbf{q}$\n",
    ">> 2. *Line search*. Choose optimal $t>0$.\n",
    ">> 3. *Update*. $\\mathbf{x}:=\\mathbf{x} + t \\Delta \\mathbf{x}$.\n",
    ">\n",
    "> **until** stopping criterion is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the gradient descent algorithm for the following system:\n",
    "$$\n",
    "\\min_{x_1,x_2}\\, \\frac{1}{2} (x_1^2 + \\gamma x_2^2)\\,\n",
    "$$\n",
    "with $\\gamma$ set to 10.\n",
    "\n",
    "![](Figures/convergence_gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 4**\n",
    "\n",
    "Complete the code for the gradient descent algorithm. Solve the previous quadratic system and compare with the analytic solution. Start at $\\mathbf{x}^{(0)}=[0,0]^\\top$. How many steps do you need for the algorithm to converge? This can be obtained by setting `trace=True`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from quadratic import gradient_descent_quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_quadratic(P, q, x0, epsilon=1e-4, trace=False):\n",
    "    \"\"\"\n",
    "    Gradient descent for quadratic systems\n",
    "\n",
    "    Inputs:\n",
    "        - P, q: the terms of the nD quadratic system\n",
    "        - x0: starting point\n",
    "        - trace: (bool) count number of steps?\n",
    "\n",
    "    Outputs:\n",
    "        - xstar: the found minimum\n",
    "        - n_steps: number of steps before algorithm terminates\n",
    "                  (if trace=True)\n",
    "    \"\"\"\n",
    "    x = x0  # initial value\n",
    "    n_steps = 0\n",
    "    while True:\n",
    "        Dx = ...  # compute GD direction\n",
    "        if ...\n",
    "            break\n",
    "        t = ...  # step size\n",
    "        ...  # perform step\n",
    "        n_steps += 1\n",
    "    if trace: return x, n_steps\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence analysis\n",
    "\n",
    "We can study the convergence of the gradient descent algorithm by using eigenvalue decomposition. The matrix $P$ can be written as:\n",
    "$$\n",
    "P = U\\Lambda U^\\top\\,,\n",
    "$$\n",
    "with\n",
    "\n",
    "- $\\Lambda=\\text{diag}(\\lambda_1,\\ldots,\\lambda_n)$, a matrix with the eigenvalues on the diagonal (sorted from small to large).\n",
    "- $U = [\\mathbf{u}_1, \\ldots, \\mathbf{u}_n]$, a matrix with the corresponding eigenvectors.\n",
    "\n",
    "Note that because $P\\succ 0$, all eigenvalues are real and positive and all eigenvectors form a real orthonormal basis.\n",
    "\n",
    "Consider the following linear transformation:\n",
    "$$\n",
    "\\mathbf{z}^{(k)}= U^\\top ( \\mathbf{x}^{\\star}-\\mathbf{x}^{(k)})\\,,\n",
    "$$\n",
    "\n",
    "which allows us to rewrite the error in closed-form:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}^{(k)}) - f(\\mathbf{x}^\\star) = \\frac{1}{2}\\sum_{i=1}^n (1-t\\lambda_i)^{2k}\\lambda_i[(\\mathbf{U}_i)^\\top(\\mathbf{x}^{(0)}-\\mathbf{x}^\\star)]^2\\,.\n",
    "$$\n",
    "\n",
    "Here, we see that:\n",
    "\n",
    "1. The error decomposes in independent terms in the eigenspace.\n",
    "2. The convergence of each term is determined by the *rate*: $|1-t\\lambda_i|$. Convergence occurs as a geometric series.\n",
    "3. The total number of steps until convergence is determined by either the smallest and largest eigenvalue.\n",
    "4. Optimal value for fixed step size is $t=\\frac{1}{\\lambda_1+\\lambda_n}$.\n",
    "\n",
    "![Illustration of eigen decomposition of the error during the iterations.](Figures/convergence_decomposition.png)\n",
    "\n",
    "Furthermore, it can be shown that if we use an exact line search for the step size, the error $f(\\mathbf{x}^{(k)}) - f(\\mathbf{x}^\\star)\\leq \\epsilon$ we need fewer than\n",
    "$$\n",
    "\\frac{\\log((f(\\mathbf{x}^{(k)}) - f(\\mathbf{x}^\\star))/\\epsilon)}{\\log(1/c)}\\,,\n",
    "$$\n",
    "with $c=1-\\frac{\\lambda_1}{\\lambda_n}<1$. The quantity $\\kappa=\\frac{\\lambda_n}{\\lambda_1}$ is called the *condition number* and largely determines the convergence. We observe:\n",
    "\n",
    "- The quality of the initial guess $f(\\mathbf{x}^{(k)}) - f(\\mathbf{x}^\\star$) has only a logarithmic impact on the number of steps required.\n",
    "- Only a few extra steps are needed to decrease $\\epsilon$ with one order of magnitude.\n",
    "- If the condition number is large, then $\\log(1/c)\\approx 1/\\kappa$. Large condition numbers require more steps.\n",
    "\n",
    "![Illustration of the convergence bounds for different condition numbers.](Figures/convergence_bound.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with momentum\n",
    "\n",
    "> *While finding the gradient of an objective function is a splendid idea, [descending] the gradient directly may not be.* ~ David J.C. MacKay\n",
    "\n",
    "Even on simple quadratic problems as discussed here, gradient descent often takes a surprisingly large number of steps to converge. This is because the gradient does not necessarily points in the general direction of the minimum. For convex problems, we are only guaranteed that the gradient points in the half-space of the minimum - a rather weak guarantee! Many improvements on gradient descent have been devised. We will briefly discuss a small modification which can lead to a very large improvement in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps with memory\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{x}^{(k+1)} = \\beta \\Delta \\mathbf{x}^{(k)} - (1-\\beta)\\nabla f(\\mathbf{x}^{(k)})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + t^{(k)}\\Delta \\mathbf{x}^{(k+1)}\\,,\n",
    "$$\n",
    "with $\\beta\\in[0,1]$ called the *momentum parameter*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm with momentum\n",
    "\n",
    "> **given** a starting point $\\mathbf{x}$,  $\\beta$\n",
    ">\n",
    "> **initialize** $\\Delta \\mathbf{x}= \\mathbf{0}$\n",
    ">\n",
    "> **repeat**\n",
    ">> 1. $\\Delta \\mathbf{x} := \\beta \\Delta \\mathbf{x}- (1-\\beta)(P\\mathbf{x}+\\mathbf{q})$\n",
    ">> 2. *Line search*. Choose optimal $t>0$.\n",
    ">> 3. *Update*. $\\mathbf{x}:=\\mathbf{x} + t \\Delta \\mathbf{x}$.\n",
    ">\n",
    "> **until** stopping criterion is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 5**\n",
    "\n",
    "1. Complete the code for gradient descent with momentum. Use it find the solution for the above system, also starting at $\\mathbf{x}=[0,0]^\\top$. Set $\\beta=0.1$. Do you see an improvement?\n",
    "\n",
    "2. Compare both algorithms for minimizing the following system:\n",
    "$$\n",
    "f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top\\begin{bmatrix}500 & 2 \\\\ 2 & 1\\end{bmatrix}\\mathbf{x} + \\begin{bmatrix}-40 \\\\100 \\end{bmatrix}^\\top\\mathbf{x} -5\\,,\n",
    "$$\n",
    "at $\\mathbf{x}_0= [0, 0]^\\top$. Does momentum increase the speed now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_quadratic_momentum(P, q, x0,\n",
    "                                        beta=0.2, epsilon=1e-4,\n",
    "                                        trace=False):\n",
    "    \"\"\"\n",
    "    Gradient descent for quadratic systems with momentum\n",
    "\n",
    "    Inputs:\n",
    "        - P, q: the terms of the nD quadratic system\n",
    "        - x0: starting point\n",
    "        - beta: momentum parameter (default set to 0.2)\n",
    "        - trace: (bool) count number of steps?\n",
    "\n",
    "    Outputs:\n",
    "        - xstar: the found minimum\n",
    "        - n_steps: number of steps before algorithm terminates (if trace=True)\n",
    "    \"\"\"\n",
    "    x = x0  # initial value\n",
    "    n_steps = 0\n",
    "    ...  # init Dx\n",
    "    while True:\n",
    "        ...  # compute gradient and search direction\n",
    "        if ...  # complete\n",
    "            break\n",
    "        t = ...  # optimal step size\n",
    "        ...  # descent step\n",
    "        n_steps += 1\n",
    "    if trace: return x, n_steps\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugated gradient descent\n",
    "\n",
    "Conjugated gradient descent is an important method for approximately minimizing quadratic systems. It converges much faster than simple gradient descent and does not have a hyperparameter as with momentum updates.\n",
    "\n",
    "The main idea is that in every step the current search direction and all previous search directions are conjugate with respect to $P$, i.e.\n",
    "\n",
    "$$\n",
    "\\Delta (\\mathbf{x}^{(k)})^\\top  P\\Delta\\mathbf{x}^{(l)}=0 \\quad\\forall  k> l\\,.\n",
    "$$\n",
    "\n",
    "The interested reader is referred to '[An Introduction to the Conjugate Gradient Method Without the Agonizing Pain](http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)' by Jonathan Richard Shewchuk for an in-depth overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: signal recovery\n",
    "\n",
    "As a practical example of minimizing quadratic systems, let us consider a simple signal recovery problem. Consider an $n$-dimensional real vector $\\mathbf{x}=[x_1,\\ldots,x_n]^\\top$. Rather than observing this vector directly, we have $m$ noisy measurements at random indices (indices drawn randomly with replacement from $\\{1,\\ldots,n\\}$): $\\mathcal{O} = \\{(i_j, y_j)\\mid j=1,\\ldots,m\\}$. These measurements are stored in an $m$-dimensional vector $\\mathbf{y}$. Can we recover $\\mathbf{x}$ from $\\mathbf{y}$?\n",
    "\n",
    "If $m<n$, then we do not have a single measurement for every element of $\\mathbf{x}$. Even if $m>n$, it is likely that some elements of $\\mathbf{x}$ are not observed due to chance (for large $n$, if $n=m$ then about 37% of the elements will not be sampled). Clearly, recovering $\\mathbf{x}$ from $\\mathbf{y}$ is an impossible assumption if we do not make some assumptions, this seems an impossible problem in general.\n",
    "\n",
    "If we assume that the different values of $\\mathbf{x}$ are on a line, then we can make a *smoothness* assumption: elements of $\\mathbf{x}$ for which the indices are close, likely will have similar values. This idea is expressed in the follow minimization problem:\n",
    "$$\n",
    "\\min_\\mathbf{x}\\, \\frac{1}{2}\\sum_{(i_j, y_j)\\in \\mathcal{O}}(y_j-{x}_{i_j})^2 + \\frac{C}{2} \\mathbf{x}^\\top K^{-1}\\mathbf{x}\\,,\n",
    "$$\n",
    "with $K^{-1}$ an inverse kernel (or covariance matrix) and $C$ a tuning hyperparameter. The matrix $K^{-1}$ encodes how the different elements of $\\mathbf{x}$ are related, constructing such a matrix is a topic in machine learning (see course Predictive Modelling). For our purposes, we have chosen this matrix as such that elements should have values closes to each other. Hence, the minimization problem has two terms:\n",
    "- a data fitting term to make sure that the recovered vector $\\mathbf{x}$ matches the observations,\n",
    "- a regularization term to ensure smoothness of the solution.\n",
    "The parameter $C$ determines the trade-off between the two terms.\n",
    "\n",
    "The problem can written purely in matrix notation by using the $(m\\times n)$ *bookkeeping matrix* $R$ for which $R_{ij}=1$ if the the $j$-th element of $\\mathbf{y}$ corresponds to the $i$-th element of $\\mathbf{x}$ and $R_{ij}=0$ otherwise. Hence, the compact matrix form is:\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{x}\\, \\frac{1}{2}(\\mathbf{y}-R\\mathbf{x})^\\top(\\mathbf{y}-R\\mathbf{x}) + \\frac{C}{2} \\mathbf{x}^\\top K^{-1}\\mathbf{x}\\,.\n",
    "$$\n",
    "\n",
    "![](Figures/signal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignments**\n",
    "\n",
    "1. Write the minimization problem in the standard form.\n",
    "2. Use the function `generate_noisy_measurements` to generate $m=100$ noisy measurements (standard deviation is 1, default) of a vector with dimensionality $n=1000$. Use the functions `make_connection_matrix` and `make_bookkeeping` to generate the associated matrices $K^{-1}$ and $L$. All are implemented in the module `signal_recovery`.\n",
    "3. Use $C=1$, generate $\\mathbf{x}^\\star$ using the closed-form solution, using gradient descent and gradient descent with momentum. How many steps do the two descent methods need to converge? Use a vector of zeros as the initial point.\n",
    "4. Minimize the system for values of $C=1\\times 10^{-2}, 1\\times 10^{-1}, 1, 10, 100$.  Use for momentum $\\beta=0, 0.1, 0.2,\\ldots, 0.9$. Make a table of the number of steps needed to reach convergence for the different values of $C$ and $\\beta$. Make a plot with the different $\\mathbf{x^\\star}$ for different values of $C$. Note that the result should be (nearly) the same whether you use momentum or not, only the number of steps will differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem in standard form**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from signal_recovery import generate_noisy_measurements, make_connection_matrix, make_bookkeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the noisy measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 noise measurements of vector of length 1000\n",
    "y, I = generate_noisy_measurements(m=100, n=1000, sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]  # measurement values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I[:10]  # positions of measurement (uniform with replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, Kinv = make_connection_matrix(n=1000)  # covariance matrix and its inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat map of K\n",
    "# values of x that are closer to each other\n",
    "# receive higher values (are expected to be similar)\n",
    "plt.imshow(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bookkeeping matrix\n",
    "# gives at which points of the vector x the measurements\n",
    "# of y correspond to\n",
    "R = make_bookkeeping(I, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.imshow(R, interpolation='nearest', cmap='binary')\n",
    "ax.set_xlabel('$i$')\n",
    "ax.set_ylabel('$j$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Boyd, S. and Vandenberghe, L., '*[Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)*'. Cambridge University Press (2004)\n",
    "- Goh, G. '[*Why Momentum really works*](https://distill.pub/2017/momentum/)' (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbpresent": {
   "slides": {
    "78c67dd2-bd77-4ded-aabf-d0853de61e2d": {
     "id": "78c67dd2-bd77-4ded-aabf-d0853de61e2d",
     "prev": "e60b6b17-f539-4bf8-b92e-96e4299a011f",
     "regions": {
      "ea780b17-16e1-4904-8d65-efdc9ce15006": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "08c91658-7ce3-4c14-b5e8-d517cae7496b",
        "part": "whole"
       },
       "id": "ea780b17-16e1-4904-8d65-efdc9ce15006"
      }
     }
    },
    "824236d6-2c58-46c1-ae5d-3c52e4fd7db9": {
     "id": "824236d6-2c58-46c1-ae5d-3c52e4fd7db9",
     "prev": "78c67dd2-bd77-4ded-aabf-d0853de61e2d",
     "regions": {
      "e5fd8f1d-50d7-4e12-81cf-30ebc0a5d884": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "02733a14-ace2-4318-8162-1bb8d6ad3522",
        "part": "whole"
       },
       "id": "e5fd8f1d-50d7-4e12-81cf-30ebc0a5d884"
      }
     }
    },
    "de3792fb-a0be-49f5-b77e-af05e6c254f1": {
     "id": "de3792fb-a0be-49f5-b77e-af05e6c254f1",
     "prev": "ffce5b51-74d9-4f59-97c9-9a4656d4281c",
     "regions": {
      "a0ad3189-e80a-4ed7-8a2a-5ed42ceb0524": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "a0ad3189-e80a-4ed7-8a2a-5ed42ceb0524"
      }
     }
    },
    "e60b6b17-f539-4bf8-b92e-96e4299a011f": {
     "id": "e60b6b17-f539-4bf8-b92e-96e4299a011f",
     "prev": "fb49b1c2-a93c-40cb-a6f8-54b15a1afee7",
     "regions": {
      "d1eb8248-b797-413f-affa-2fa4237695b0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1dae18cf-7a2c-4a39-9d5e-fb36e150aba7",
        "part": "whole"
       },
       "id": "d1eb8248-b797-413f-affa-2fa4237695b0"
      }
     }
    },
    "fb49b1c2-a93c-40cb-a6f8-54b15a1afee7": {
     "id": "fb49b1c2-a93c-40cb-a6f8-54b15a1afee7",
     "layout": "grid",
     "prev": "de3792fb-a0be-49f5-b77e-af05e6c254f1",
     "regions": {
      "7296524e-f379-47c1-85e8-5073b3baf51b": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.5,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "8c3f8673-19f1-483f-b27d-fc1edcc0843e",
        "part": "source"
       },
       "id": "7296524e-f379-47c1-85e8-5073b3baf51b"
      },
      "c748d519-d671-4a46-ba90-371d28d7112c": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.5,
        "x": 0.5,
        "y": 0
       },
       "content": {
        "cell": "02733a14-ace2-4318-8162-1bb8d6ad3522",
        "part": "source"
       },
       "id": "c748d519-d671-4a46-ba90-371d28d7112c"
      }
     }
    },
    "ffce5b51-74d9-4f59-97c9-9a4656d4281c": {
     "id": "ffce5b51-74d9-4f59-97c9-9a4656d4281c",
     "prev": null,
     "regions": {}
    }
   },
   "themes": {
    "default": "39491860-1114-428e-824a-dd298c10bdf6",
    "theme": {
     "39491860-1114-428e-824a-dd298c10bdf6": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "39491860-1114-428e-824a-dd298c10bdf6",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         0,
         0,
         0
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         0,
         0,
         139
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         0,
         0,
         0
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
